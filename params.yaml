data_processing:
  max_retries: 3
  wait_time: 2
  batch_size: 10  # For large PDF processing

# General settings
preprocessing:
  correct_spelling: true  # Flag to enable/disable spelling correction

text_preprocessing:
  remove_html: true            # Option to remove HTML
  remove_urls: true            # Option to remove URLs
  remove_emails: true          # Option to remove email addresses
  handle_accents: true         # Option to normalize accented characters
  remove_unicode_chars: true   # Option to remove non-ASCII characters
  remove_punctuations: true    # Option to remove punctuation
  remove_digits: false         # Option to remove digits
  remove_extra_spaces: true    # Option to remove extra spaces
  correct_spelling: true 
  expand_contractions: true

# Keyword extraction settings for YAKE
keyword_extraction:
  remove_newline: true
  stop_words: true
  language: "en"
  max_ngram_size: 3
  deduplication_threshold: 0.75
  deduplication_algo: "seqm3"
  window_size: 2

# Rapidfuzz settings
rapidfuzz:
  threshold: 55  # Threshold for deduplication in Rapidfuzz


text_processing:
  batch_size: 500                # Batch size for processing phrases
  dynamic_percentile: 20         # Percentile for dynamic threshold calculation
  max_tokens: 4096               # Maximum token limit for semantic chunking
  embedding_model_name: "longformer_model"  # Replace with the embedding model name
  embedding_tokenizer: "longformer_tokenizer"  # Replace with the embedding model name
  sentence_model: "sentence_model"
  tokenizer_max_length: 4096     # Maximum length for tokenizer
  device: "cuda"                 # Device for model processing, e.g., 'cuda' or 'cpu'
  padding: "max_length"          # Padding strategy for tokenizer
  truncation: true

query_generation:
  most_common_words_count: 10 # n in get_most_common_words
  num_topics: 5 # num_topics in extract_topics

text_summarization:
  device: "cuda" 
  chunking_top_k: 10  
  reranking: 
    top_k: 5
    rerank_model: "rerank_model"
    rerank_tokenizer: "rerank_tokenizer"

  summarization:
    summarization_model: "distilbart_model"
    summarization_tokenizer: "distilbart_tokenizer"
    max_length: 200
    min_length: 50
    tokenizer_max_length: 1024